from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages

class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str      # ç»è¿‡LLMç†è§£åçš„ç”¨æˆ·éœ€æ±‚æ€»ç»“
    search_query: str    # ä¼˜åŒ–åç”¨äºTavily APIçš„æœç´¢æŸ¥è¯¢
    search_results: str  # Tavilyæœç´¢è¿”å›çš„ç»“æœ
    final_answer: str    # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
    step: str            # æ ‡è®°å½“å‰æ­¥éª¤

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from openai import OpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from tavily import TavilyClient

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–æ¨¡å‹
# æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª llm å®ä¾‹æ¥é©±åŠ¨æ‰€æœ‰èŠ‚ç‚¹çš„æ™ºèƒ½



llm = ChatOpenAI(
    api_key=os.environ.get('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com")

response = llm.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def understand_query_node(state: SearchState) -> dict:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯"""
    user_message = state["messages"][-1].content
    
    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    response = llm.invoke([SystemMessage(content=understand_prompt)])
    response_text = response.content
    
    # è§£æLLMçš„è¾“å‡ºï¼Œæå–æœç´¢å…³é”®è¯
    search_query = user_message # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()
    
    return {
        "user_query": response_text,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘å°†ä¸ºæ‚¨æœç´¢ï¼š{search_query}")]
    }

def tavily_search_node(state: SearchState) -> dict:
    """æ­¥éª¤2ï¼šä½¿ç”¨Tavily APIè¿›è¡ŒçœŸå®æœç´¢"""
    search_query = state["search_query"]
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        response = tavily_client.search(
            query=search_query, search_depth="basic", max_results=5, include_answer=True
        )
        # ... (å¤„ç†å’Œæ ¼å¼åŒ–æœç´¢ç»“æœ) ...
        search_results = ... # æ ¼å¼åŒ–åçš„ç»“æœå­—ç¬¦ä¸²
        
        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content="âœ… æœç´¢å®Œæˆï¼æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")]
        }
    except Exception as e:
        # ... (å¤„ç†é”™è¯¯) ...
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{e}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜...")]
        }

def generate_answer_node(state: SearchState) -> dict:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    if state["step"] == "search_failed":
        # å¦‚æœæœç´¢å¤±è´¥ï¼Œæ‰§è¡Œå›é€€ç­–ç•¥ï¼ŒåŸºäºLLMè‡ªèº«çŸ¥è¯†å›ç­”
        fallback_prompt = f"æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\nç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}"
        response = llm.invoke([SystemMessage(content=fallback_prompt)])
    else:
        # æœç´¢æˆåŠŸï¼ŒåŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}
æœç´¢ç»“æœï¼š\n{state['search_results']}
è¯·ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”..."""
        response = llm.invoke([SystemMessage(content=answer_prompt)])
    
    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver

def create_search_assistant():
    workflow = StateGraph(SearchState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)
    
    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)
    
    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app
